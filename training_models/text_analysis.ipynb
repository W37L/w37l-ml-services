{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 184354 entries, 0 to 184353\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   is_offensive  184354 non-null  int64 \n",
      " 1   text          184350 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#Reading the data\n",
    "df = pd.read_csv('toxic_comments.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows missing data in 'is_offensive' but have data in 'text':\n",
      "Empty DataFrame\n",
      "Columns: [is_offensive, text]\n",
      "Index: []\n",
      "Rows missing data in 'text' but have data in 'is_offensive':\n",
      "        is_offensive text\n",
      "102193             0  NaN\n",
      "122526             0  NaN\n",
      "154264             0  NaN\n",
      "170295             0  NaN\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values\n",
    "missing_in_offensive = df[df['is_offensive'].isna() & ~df['text'].isna()]\n",
    "missing_in_text = df[~df['is_offensive'].isna() & df['text'].isna()]\n",
    "\n",
    "print(f\"Rows missing data in 'is_offensive' but have data in 'text':\\n{missing_in_offensive}\")\n",
    "print(f\"Rows missing data in 'text' but have data in 'is_offensive':\\n{missing_in_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 184350 entries, 0 to 184353\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   is_offensive  184350 non-null  int64 \n",
      " 1   text          184350 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Droping rows with missing values\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the text\n",
    "def preprocess_text(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    review_without_stop_words = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    review_stemmed = ' '.join([stemmer.stem(word) for word in review_without_stop_words.split()])\n",
    "    return review_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/7gwfh58127s35rffsw8410lr0000gn/T/ipykernel_91095/3488910226.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for text in df['text']:\n",
    "    df_list.append(text)\n",
    "\n",
    "df_preprocessed = []\n",
    "for text in df_list:\n",
    "    df_preprocessed.append(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184350, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Creating a bag of words\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "#Fiting and transforming the data\n",
    "feature = vectorizer.fit_transform(df_preprocessed).toarray()\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (132732, 10000) (132732,)\n",
      "Validation set: (14748, 10000) (14748,)\n",
      "Test set: (36870, 10000) (36870,)\n"
     ]
    }
   ],
   "source": [
    "#Spliting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature, df['is_offensive'], test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilianobusso/Code projects/w37l-ml-services/venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.949891510713317\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "model = LinearSVC(dual=True, max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "validation_score = model.score(X_val, y_val)\n",
    "print(\"Validation accuracy:\", validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being profane: [1]\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "new_text = \"chineese people are dirty and disgusting\"\n",
    "proccesed = preprocess_text(new_text)\n",
    "new_features = vectorizer.transform([proccesed])\n",
    "prediction = model.predict(new_features)\n",
    "print(f\"Probability of being profane: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Wow! What an incredible movie! The plot was engaging\n",
      "  - Predicted Profanity: No profanity detected (Probability: 0)\n",
      "Sentence: I absolutely loved this food!\n",
      "  - Predicted Profanity: No profanity detected (Probability: 0)\n",
      "Sentence: Great job on the project, everyone!\n",
      "  - Predicted Profanity: No profanity detected (Probability: 0)\n",
      "Sentence: Thank you for your hard work!\n",
      "  - Predicted Profanity: No profanity detected (Probability: 0)\n",
      "Sentence: I hate Europeans, they are idiots\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n",
      "Sentence: Americans are the worst, they are so stupid\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n",
      "Sentence: I hate white people, they are so dirty\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n",
      "Sentence: Are you mental insane or something?\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n",
      "Sentence: Go back to your country, mother fucker\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n",
      "Sentence: Americans are dirty and disgusting\n",
      "  - Predicted Profanity: Profanity detected (Probability: 1)\n"
     ]
    }
   ],
   "source": [
    "# Method to classify sentences\n",
    "def classify_sentences(sentences, vectorizer, model):\n",
    "    text_preprocessed = []\n",
    "    for text in sentences:\n",
    "        text_preprocessed.append(preprocess_text(text))\n",
    "    \n",
    "    vectors = vectorizer.transform(text_preprocessed).toarray()\n",
    "    predictions = model.predict(vectors)\n",
    "    \n",
    "    for sentence, prediction in zip(sentences, predictions):\n",
    "        profanity = \"Profanity detected\" if prediction == 1 else \"No profanity detected\"\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"  - Predicted Profanity: {profanity} (Probability: {prediction})\")\n",
    "\n",
    "# Test Example sentences\n",
    "input_sentences = [\n",
    "    \"Wow! What an incredible movie! The plot was engaging\",\n",
    "    \"I absolutely loved this food!\",\n",
    "    \"Great job on the project, everyone!\",\n",
    "    \"Thank you for your hard work!\",\n",
    "\n",
    "    \"I hate Europeans, they are idiots\",\n",
    "    \"Americans are the worst, they are so stupid\",\n",
    "    \"I hate white people, they are so dirty\",\n",
    "    \"Are you mental insane or something?\",\n",
    "    \"Go back to your country, mother fucker\",\n",
    "    \"Americans are dirty and disgusting\"\n",
    "]\n",
    "\n",
    "classify_sentences(input_sentences, vectorizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../src/mlservice/service/ml_models/count_vectorizer.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model to joblib file\n",
    "model_folder = '../src/mlservice/service/utils'\n",
    "joblib.dump(model, f'{model_folder}/linear_svc_model.joblib')\n",
    "joblib.dump(vectorizer, f'{model_folder}/count_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "# Testing joblib file\n",
    "model = joblib.load(f'{model_folder}/linear_svc_model.joblib')\n",
    "vectorizer = joblib.load(f'{model_folder}/count_vectorizer.joblib')\n",
    "new_text = \"chineese people are dirty and disgusting\"\n",
    "prep_text = preprocess_text(new_text)\n",
    "new_feat = vectorizer.transform([prep_text])\n",
    "prediction = model.predict(new_feat)\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
